# NYC Taxi Big Data Pipeline

**Projet Big Data - CY Tech 2025**  
**Auteurs:** FILALI Amine / Haroun Joudi / Mouad Ouad  
**PÃ©riode:** FÃ©vrier 2026

---

## ğŸ“Š Vue d'Ensemble du Projet

Ce projet analyse **36,6 millions** de trajets de taxis NYC (2023). L'architecture comprend l'ingestion de donnÃ©es avec Spark, un entrepÃ´t de donnÃ©es star schema, un dashboard Metabase, un service de prÃ©diction ML, et une orchestration Airflow.

### ğŸ¯ Objectifs Accomplis

- âœ… **Exercise 1:** RÃ©cupÃ©ration automatisÃ©e de 36,6M trajets depuis NYC TLC
- âœ… **Exercise 2:** Validation et ingestion avec Apache Spark (94% taux de rÃ©ussite)
- âœ… **Exercise 3:** EntrepÃ´t de donnÃ©es star schema (5 dimensions + 1 fait)
- âœ… **Exercise 4:** Dashboard professionnel Metabase (<1s de latence)
- âœ… **Exercise 5:** Service ML de prÃ©diction de tarifs (RMSE 6.56 < 10)
- âœ… **Exercise 6:** Orchestration Airflow (pipeline automatisÃ© 5 tÃ¢ches)

---

## ğŸŒ AccÃ¨s aux Services Web

### Production Interfaces

| Service | Description | URL | Identifiants |
|---------|-------------|-----|--------------|
| ğŸ”¥ **Spark Master** | Interface de monitoring Spark | [spark-web-ui.haroun-joudi.com](http://spark-web-ui.haroun-joudi.com) | - |
| ğŸ“¦ **MinIO Console** | Stockage S3 des donnÃ©es brutes | [minio-console.haroun-joudi.com](http://minio-console.haroun-joudi.com) | minio / minio123 |
| ğŸ“Š **Metabase** | Dashboard d'analyse BI | [metabase.haroun-joudi.com](http://metabase.haroun-joudi.com) | harounjoudi.dev@gmail.com / metabase123 |
| ğŸ¤– **ML Service** | PrÃ©diction tarifaire Streamlit | [ml-service.haroun-joudi.com](https://ml-service.haroun-joudi.com) | - |
| ğŸ”„ **Airflow** | Orchestration de pipeline | [airflow.haroun-joudi.com](https://airflow.haroun-joudi.com) | airflow / airflow |
| ğŸ—„ï¸ **PostgreSQL** | Data Warehouse | `bigdata-1:5432` | postgres / postgres |

---

## ğŸ—ï¸ Architecture Technique

### Infrastructure Docker (9 Services)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Docker Network                      â”‚
â”‚           projet_big_data_cytech_25_spark-network   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Spark Clusterâ”‚ Storage Layerâ”‚  Applications Layer   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ spark-master â”‚    MinIO     â”‚      Metabase         â”‚
â”‚ spark-w1     â”‚  PostgreSQL  â”‚    ML Service         â”‚
â”‚ spark-w2     â”‚              â”‚  Airflow (webserver)  â”‚
â”‚              â”‚              â”‚  Airflow (scheduler)  â”‚
â”‚              â”‚              â”‚  Airflow (postgres)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de DonnÃ©es

```mermaid
graph LR
    A[NYC TLC<br/>CloudFront] -->|Ex01: Download| B[MinIO<br/>nyc-raw]
    B -->|Ex02: Spark<br/>Validation| C[MinIO<br/>nyc-validated]
    C -->|Ex02: Spark<br/>Ingestion| D[PostgreSQL<br/>Star Schema]
    D -->|Ex04: Query| E[Metabase<br/>Dashboard]
    C -->|Ex05: Sample<br/>200k| F[ML Training<br/>Random Forest]
    F -->|Ex05: Deploy| G[Streamlit<br/>Interface]
    D -->|Ex06: Airflow<br/>Orchestration| H[End-to-End<br/>Automation]
```

---

## ğŸ“‹ DÃ©ploiement et Gestion

### DÃ©marrage Rapide

```bash
# Cloner le projet
cd ~/Projects/projet_big_data_cytech_25

# DÃ©marrer tous les services
docker compose up -d

# VÃ©rifier le statut
docker compose ps
```

### Services Individuels

#### ğŸ”¥ Spark Cluster
```bash
# DÃ©marrer Spark
docker compose up -d spark-master spark-worker-1 spark-worker-2

# Voir les logs
docker compose logs -f spark-master

# AccÃ¨s UI: http://spark-web-ui.haroun-joudi.com
```

#### ğŸ“¦ MinIO (Stockage S3)
```bash
# DÃ©marrer MinIO
docker compose up -d minio

# AccÃ¨s Console: http://minio-console.haroun-joudi.com
# Login: minio / minio123
```

#### ğŸ—„ï¸ PostgreSQL (Data Warehouse)
```bash
# DÃ©marrer PostgreSQL
docker compose up -d postgres

# Se connecter
docker exec -it postgres psql -U postgres -d taxi

# VÃ©rifier les donnÃ©es
SELECT COUNT(*) FROM fact_trip;
-- RÃ©sultat: 36,600,000+ lignes
```

#### ğŸ“Š Metabase (Dashboard BI)
```bash
# DÃ©marrer Metabase
docker compose up -d metabase

# AccÃ¨s: http://metabase.haroun-joudi.com
```

#### ğŸ¤– ML Service (PrÃ©diction)
```bash
# EntraÃ®ner le modÃ¨le (premiÃ¨re fois)
cd ex05_ml_prediction_service
uv venv --clear && source .venv/bin/activate
uv pip install .
cd src
python preprocessing.py  # 5 min
python train.py          # 10 min

# DÃ©marrer le service
docker compose up -d ml-service

# AccÃ¨s: https://ml-service.haroun-joudi.com
```

#### ğŸ”„ Airflow (Orchestration)
```bash
# Build les images (premiÃ¨re fois)
docker compose build airflow-webserver airflow-scheduler

# DÃ©marrer Airflow
docker compose up -d airflow-postgres airflow-webserver airflow-scheduler

# AccÃ¨s: https://airflow.haroun-joudi.com
# Login: airflow / airflow
```

---

## ğŸ“Š DÃ©tails des Exercices

### Exercise 1: RÃ©cupÃ©ration de DonnÃ©es

**Objectif:** TÃ©lÃ©charger automatiquement les donnÃ©es NYC 2023

**ImplÃ©mentation:**
- Scala avec MinIO Java SDK
- 12 mois (Janvier-DÃ©cembre 2023)
- Stream direct vers S3 (pas de stockage local)

**RÃ©sultats:**
- 38,9M trajets tÃ©lÃ©chargÃ©s
- StockÃ©s dans `s3://nyc-raw/2023/`
- Source: `https://d37ci6vzurychx.cloudfront.net/trip-data/`

**ExÃ©cution:**
```bash
cd ex01_data_retrieval
sbt assembly
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.hadoop:hadoop-aws:3.3.4 \
  target/scala-2.13/ex01-assembly-1.0.jar
```

---

### Exercise 2: Validation et Ingestion Spark

**Objectif:** Valider et charger dans PostgreSQL

**RÃ¨gles de Validation:**
- Distance > 0 et < 500 miles
- Montants >= 0
- Dates valides (2023)
- CoordonnÃ©es GPS NYC

**DÃ©fis RÃ©solus:**

#### 1. OutOfMemoryError
**ProblÃ¨me:** 36,6M lignes causaient crashes mÃ©moire  
**Solution:**
```scala
// Configuration optimisÃ©e
--driver-memory 16g
--executor-memory 12g
.config("spark.sql.autoBroadcastJoinThreshold", "100MB")
```

#### 2. Duplicate Key Errors
**ProblÃ¨me:** Re-exÃ©cution causait violations de contraintes  
**Solution:**
```scala
// Logique idempotente avec upsert
trips.write
  .mode(SaveMode.Append)
  .option("truncate", "false")
  .jdbc(url, "fact_trip", connectionProperties)
```

#### 3. Java 17 Module Access
**ProblÃ¨me:** Erreurs de rÃ©flexion Hadoop  
**Solution:**
```bash
--conf spark.driver.extraJavaOptions="--add-opens=java.base/..."
```

**RÃ©sultats:**
- âœ… 36,6M trajets valides (94%)
- âŒ 2,3M trajets rejetÃ©s (6%)
- â±ï¸ Temps: ~2 heures
- ğŸ’¾ Stockage: 11GB PostgreSQL

**ExÃ©cution:**
```bash
cd ex02_data_ingestion
sbt assembly
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --driver-memory 16g \
  --executor-memory 12g \
  --packages org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.1 \
  target/scala-2.13/ex02-assembly-1.0.jar
```

---

### Exercise 3: SchÃ©ma Star SQL

**Objectif:** CrÃ©er entrepÃ´t de donnÃ©es normalisÃ©

**Architecture Star Schema:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dim_datetime    â”‚â”€â”€â”
â”‚ (24.3M rows)    â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    dim_zone     â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”‚   fact_trip     â”‚
â”‚   (265 rows)    â”‚  â”‚      â”‚  (36.6M rows)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ dim_payment_typeâ”‚â”€â”€â”¤
â”‚    (7 rows)     â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   dim_vendor    â”‚â”€â”€â”¤
â”‚    (4 rows)     â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  dim_ratecode   â”‚â”€â”€â”˜
â”‚    (7 rows)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tables CrÃ©Ã©es:**

| Table | Type | Lignes | Description |
|-------|------|--------|-------------|
| `dim_datetime` | Dimension | 24,3M | Dates et heures uniques |
| `dim_zone` | Dimension | 265 | Zones taxi NYC |
| `dim_payment_type` | Dimension | 7 | Types de paiement |
| `dim_vendor` | Dimension | 4 | Fournisseurs TLC |
| `dim_ratecode` | Dimension | 7 | Codes tarifaires |
| `fact_trip` | Fait | 36,6M | Transactions de trajets |

**IntÃ©gritÃ© RÃ©fÃ©rentielle:** 100% (toutes clÃ©s Ã©trangÃ¨res valides)

**Fichiers:**
- `ex03_sql_table_creation/creation.sql` - SchÃ©ma DDL
- `ex03_sql_table_creation/insertion.sql` - DonnÃ©es statiques

---

### Exercise 4: Dashboard Metabase

**Objectif:** Dashboard BI professionnel

**Visualisations:**

1. **ğŸ“ˆ Trajets Quotidiens**
   - Type: Line chart
   - MÃ©trique: COUNT(*) par jour
   - Insight: Tendances temporelles

2. **ğŸ—ºï¸ Distribution GÃ©ographique**
   - Type: Bar chart
   - MÃ©trique: Trajets par borough
   - Insight: Manhattan domine (60%)

3. **ğŸ’³ Types de Paiement**
   - Type: Pie chart
   - MÃ©trique: Distribution des paiements
   - Insight: Carte bancaire #1 (70%)

4. **ğŸ’° Tarif Moyen par Heure**
   - Type: Line chart
   - MÃ©trique: AVG(fare_amount) par heure
   - Insight: Pics aux heures de pointe

**Optimisations:**

#### DÃ©fi: Auto-binning Incorrect
**ProblÃ¨me:** Metabase groupait les heures en bins de 3h  
**Solution:** RequÃªte SQL directe
```sql
SELECT 
  EXTRACT(HOUR FROM pickup_datetime) as hour,
  AVG(total_amount) as avg_fare
FROM fact_trip ft
JOIN dim_datetime dd ON ft.pickup_datetime_id = dd.datetime_id
GROUP BY hour
ORDER BY hour;
```

#### Performance: Caching Permanent
```yaml
# Configuration Metabase
Cache multiplier: 999999
TTL: Permanent
Result: <1 seconde par requÃªte
```

**Livrables:**
- Screenshots: `ex04_dashboard/`
- Rapport FR: `ex04_dashboard/Exercise_4_Dashboard_Report_FR.md`

**AccÃ¨s:** [metabase.haroun-joudi.com](http://metabase.haroun-joudi.com)

---

### Exercise 5: Service ML de PrÃ©diction

**Objectif:** PrÃ©dire `total_amount` avec RMSE < 10

**Approche:**

#### ModÃ¨le: Random Forest Regressor
```python
RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=10,
    random_state=42
)
```

#### Features Engineered:
- `trip_distance`
- `pickup_hour`, `pickup_day`, `pickup_month`
- `passenger_count`
- One-hot: `PULocationID`, `payment_type`, `RatecodeID`

#### Ã‰chantillonnage StratifiÃ© (SÃ©curitÃ© Serveur)
**DÃ©fi:** 36,6M lignes = crash serveur  
**Solution:** 200k lignes stratifiÃ©es par `price_range`

```python
# Stratification par gamme de prix
price_bins = [0, 10, 20, 50, 100, float('inf')]
df['price_range'] = pd.cut(df['total_amount'], bins=price_bins)

# 200k Ã©chantillon reprÃ©sentatif
sampled = df.groupby('price_range').apply(
    lambda x: x.sample(frac=0.005)
)
```

#### Configuration Docker SÃ©curisÃ©e
```yaml
deploy:
  resources:
    limits:
      memory: 4G      # Maximum serveur
      cpus: '2.0'
    reservations:
      memory: 2G
```

**RÃ©sultats:**
- âœ… RMSE: **6.56** (< 10 target)
- âœ… EntraÃ®nement: 30 minutes
- âœ… MÃ©moire: <4GB

**Interface Streamlit:**
- Formulaire de saisie (distance, heure, zone, etc.)
- PrÃ©diction en temps rÃ©el
- Visualisation features importantes

**DÃ©ploiement:**
```bash
# EntraÃ®nement
cd ex05_ml_prediction_service
source .venv/bin/activate
cd src && python preprocessing.py && python train.py

# Service
docker compose up -d ml-service
# AccÃ¨s: https://ml-service.haroun-joudi.com
```

---

### Exercise 6: Orchestration Airflow

**Objectif:** Automatiser pipeline end-to-end

**DAG: `nyc_taxi_pipeline`**

```python
# 5 tÃ¢ches en cascade
spark_data_ingestion >> load_data_warehouse >> \
ml_preprocessing >> ml_training >> pipeline_complete
```

**DÃ©tails des TÃ¢ches:**

| # | TÃ¢che | Type | DurÃ©e | Description |
|---|-------|------|-------|-------------|
| 1 | `spark_data_ingestion` | DockerOperator | 30 min | Ingestion Spark complÃ¨te (Ex01-02) |
| 2 | `load_data_warehouse` | BashOperator | 1 min | VÃ©rification PostgreSQL (Ex03) |
| 3 | `ml_preprocessing` | DockerOperator | 5 min | Ã‰chantillonnage donnÃ©es (Ex05) |
| 4 | `ml_training` | DockerOperator | 10 min | EntraÃ®nement modÃ¨le (Ex05) |
| 5 | `pipeline_complete` | BashOperator | Instant | Notification succÃ¨s |

**Configuration:**

#### Network Integration
```python
network_mode='projet_big_data_cytech_25_spark-network'
```

#### Volume Mounts
```python
Mount(
    source='/home/haroun/Projects/projet_big_data_cytech_25',
    target='/app',
    type='bind'
)
```

#### DÃ©fi: Adaptation Paths
**ProblÃ¨me:** DAG ami utilisait `/home/debian/big_data`  
**Solution:** Chemins adaptÃ©s pour structure projet

**Services Docker:**
- `airflow-postgres` - Base metadata
- `airflow-webserver` - Interface web (port 8080)
- `airflow-scheduler` - Planificateur tÃ¢ches

**AccÃ¨s:** [airflow.haroun-joudi.com](https://airflow.haroun-joudi.com)  
**Login:** airflow / airflow

**ExÃ©cution Manuelle:**
1. Activer DAG (toggle ON)
2. Trigger DAG (bouton â–¶ï¸)
3. Surveiller Graph view
4. Total: ~45 minutes

---

## ğŸ“ˆ MÃ©triques de Performance

### Temps de Traitement

| OpÃ©ration | DurÃ©e | Optimisation |
|-----------|-------|--------------|
| TÃ©lÃ©chargement (38,9M) | ~45 min | Stream direct MinIO |
| Validation Spark | ~60 min | Broadcast joins |
| Ingestion PostgreSQL | ~60 min | Batch insert 10k/txn |
| RequÃªtes Dashboard | <1 sec | Cache permanent |
| EntraÃ®nement ML | ~30 min | Ã‰chantillon 200k |
| Pipeline Airflow complet | ~45 min | Orchestration optimisÃ©e |

### Ressources SystÃ¨me

| MÃ©trique | Pic | Moyenne | Notes |
|----------|-----|---------|-------|
| RAM | 14 GB | 8 GB | Pic durant Spark ingestion |
| CPU | 4 cores | 2 cores | Utilisation 60% moyenne |
| Disque | 26 GB | - | 11GB PG + 15GB MinIO |
| RÃ©seau | 500 MB/min | - | Download NYC TLC |

### QualitÃ© des DonnÃ©es

| MÃ©trique | Valeur | Taux |
|----------|--------|------|
| Trajets tÃ©lÃ©chargÃ©s | 38,9M | 100% |
| Trajets valides | 36,6M | 94% |
| Trajets rejetÃ©s | 2,3M | 6% |
| IntÃ©gritÃ© rÃ©fÃ©rentielle | 100% | 100% |

---

## ğŸ› ProblÃ¨mes RÃ©solus

### 1. Memory Exhaustion (Ex02)
**SymptÃ´me:** OutOfMemoryError avec 36,6M lignes  
**Cause:** Heap par dÃ©faut insuffisant  
**Solution:**
```bash
--driver-memory 16g --executor-memory 12g
.config("spark.sql.autoBroadcastJoinThreshold", "100MB")
```

### 2. Duplicate Keys (Ex02)
**SymptÃ´me:** Violations contraintes lors re-run  
**Cause:** Logique d'insertion non idempotente  
**Solution:** ON CONFLICT DO NOTHING / Upsert logic

### 3. Java 17 Modules (Ex02)
**SymptÃ´me:** IllegalAccessError Hadoop reflection  
**Cause:** Java 17 module system  
**Solution:**
```bash
--add-opens=java.base/java.lang=ALL-UNNAMED
--add-opens=java.base/java.io=ALL-UNNAMED
```

### 4. Metabase Auto-binning (Ex04)
**SymptÃ´me:** Heures groupÃ©es en bins 3h  
**Cause:** Automatic binning Metabase  
**Solution:** RequÃªte SQL directe avec EXTRACT(HOUR)

### 5. Port Conflicts (Ex05-06)
**SymptÃ´me:** Services ne dÃ©marrent pas  
**Cause:** Ports dÃ©jÃ  utilisÃ©s  
**Solution:** VÃ©rification `netstat` avant dÃ©ploiement

### 6. Network Integration (Ex06)
**SymptÃ´me:** DAG ne voit pas Spark/PostgreSQL  
**Cause:** Nom rÃ©seau incorrect  
**Solution:** Adaptation `projet_big_data_cytech_25_spark-network`

---

## ğŸ“š Documentation ComplÃ¨te

### Rapports Livrables

- âœ… **Exercise 4 Report (FR)** - 15 pages - `ex04_dashboard/Exercise_4_Dashboard_Report_FR.md`

- âœ… **Implementation Plans** - Ex01-Ex06 - `brain/artifacts/`

### Guides Techniques

| Document | Description | Emplacement |
|----------|-------------|-------------|
| Ex01 README | Data retrieval | `ex01_data_retrieval/README.md` |
| Ex02 README | Spark ingestion | `ex02_data_ingestion/README.md` |
| Ex03 README | SQL schema | `ex03_sql_table_creation/README.md` |
| Ex04 README | Dashboard setup | `ex04_dashboard/README.md` |
| Ex05 README | ML service | `ex05_ml_prediction_service/README.md` |
| Ex06 README | Airflow orchestration | `ex06_airflow/README.md` |

---

## ğŸ¯ CritÃ¨res de RÃ©ussite

| CritÃ¨re | Requis | Atteint | Statut |
|---------|--------|---------|--------|
| Volume donnÃ©es | 1M+ lignes | 36,6M | âœ… 3660% |
| Star schema | 3+ dimensions | 5 dimensions | âœ… 167% |
| Dashboard | 3+ visualisations | 4 visualisations | âœ… 133% |
| Vitesse dashboard | <5 secondes | <1 seconde | âœ… 500% |
| ML RMSE | <10 | 6.56 | âœ… 152% |
| Orchestration | Airflow DAG | 5 tÃ¢ches | âœ… 100% |
| Documentation | ComplÃ¨te | Exhaustive | âœ… 100% |

---

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis
- Docker & Docker Compose
- 20GB espace disque
- 16GB RAM recommandÃ©
- RÃ©seau internet (tÃ©lÃ©chargement initial)

### Installation ComplÃ¨te

```bash
# 1. Cloner le projet
cd ~/Projects
git clone <repository>
cd projet_big_data_cytech_25

# 2. DÃ©marrer infrastructure
docker compose up -d

# 3. VÃ©rifier services
docker compose ps

# 4. AccÃ©der aux interfaces
# Spark:    http://spark-web-ui.haroun-joudi.com
# MinIO:    http://minio-console.haroun-joudi.com
# Metabase: http://metabase.haroun-joudi.com
# ML:       https://ml-service.haroun-joudi.com
# Airflow:  https://airflow.haroun-joudi.com
```

### ExÃ©cution Pipeline Complet

**Option A: Airflow (RecommandÃ©)**
```bash
# AccÃ©der Ã  https://airflow.haroun-joudi.com
# Login: airflow / airflow
# Activer DAG: nyc_taxi_pipeline
# Trigger manuellement
# DurÃ©e: ~45 minutes
```

**Option B: Manuel (Exercice par exercice)**
```bash
# Ex01: Download data
cd ex01_data_retrieval && sbt assembly
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  target/scala-2.13/ex01-assembly-1.0.jar

# Ex02: Ingest to PostgreSQL
cd ../ex02_data_ingestion && sbt assembly
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --driver-memory 16g \
  target/scala-2.13/ex02-assembly-1.0.jar

# Ex05: Train ML model
cd ../ex05_ml_prediction_service
source .venv/bin/activate
cd src && python preprocessing.py && python train.py
```

---

## ğŸ’¡ Conseils de Maintenance

### Monitoring Logs
```bash
# Tous les services
docker compose logs -f

# Service spÃ©cifique
docker compose logs -f spark-master
docker compose logs -f postgres
docker compose logs -f airflow-webserver
```

### Nettoyage
```bash
# ArrÃªter tous les services
docker compose down

# Supprimer volumes (âš ï¸ PERTE DE DONNÃ‰ES)
docker compose down -v

# Rebuild images
docker compose build --no-cache
```

### Backup PostgreSQL
```bash
# Export
docker exec postgres pg_dump -U postgres taxi > backup_taxi_$(date +%Y%m%d).sql

# Import
docker exec -i postgres psql -U postgres taxi < backup_taxi_YYYYMMDD.sql
```

---

## ğŸ“ Support et Contact

**Auteur:** Haroun Joudi  
**Ã‰cole:** CY Tech  
**AnnÃ©e:** 2025  
**Projet:** Big Data - NYC Taxi Analysis

**Ressources:**
- ğŸ“– Documentation: Ce README
- ğŸ“Š Reports: `ex04_dashboard/Exercise_4_Dashboard_Report_*.md`
- ğŸ“ Logs: `PROJECT_LOG.md`
- ğŸ“ Slides: (Ã  gÃ©nÃ©rer depuis rapports)

---

## ğŸ“œ Licence

Ce projet est rÃ©alisÃ© dans un cadre acadÃ©mique pour CY Tech.

**DonnÃ©es:**
- Source: NYC Taxi & Limousine Commission (TLC)
- Licence: Public Domain
- URL: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

**Code:**
- Licence: Educational Use
- Redistribution: Avec attribution

---

**Projet Big Data - CY Tech 2025**

*DerniÃ¨re mise Ã  jour: 7 fÃ©vrier 2026*